경사하강법
===

### 1. 경사하강법이란?
* 비용 함수가 최소가 되는 W 파라미터를 구할 때, W 파라미터 개수가 적으면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있지만 많아지면 고차원 방정식으로도 풀기 어려움
* 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 방식. '데이터를 기반으로 알고리즘이 스스로 학습한다'는 머신러닝 개념을 가능하게 만들어준 핵심 기법 중 하나
* 경사 하강법의 사전적 의미인 '점진적인 하강'에서도 알 수 있듯이, '점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트 하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식
* 경사하강법은 반복적으로 비용 함수의 반환 값(예측 값과 실제 값의 차이)가 작아지는 방향성을 가지고 W 파라미터를 지속적으로 보정해 나감. 오류값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W값을 최적 파라미터로 반환함

### 2. 경사하강법 특징
* 핵심 : 어떻게 하면 오류가 작아지는 방향으로 W값을 보정할 수 있을까?
* 비용 함수를 미분해서 미분 함수의 최소값을 구해야 함 (편미분 사용)
  ![image](https://github.com/seungye-kwak/til_log/assets/112370282/dccef469-898b-4df4-ac6b-88c2b50e5211)
  ![image](https://github.com/seungye-kwak/til_log/assets/112370282/c5c53b7e-f4f9-4928-9169-90c5f1e13567)

